import asyncio
import time
import hashlib
from typing import Dict, List, Optional, Set
from dataclasses import dataclass
from datetime import datetime
import aiohttp
from bs4 import BeautifulSoup
from loguru import logger

from config.settings import settings
from ai.analyzer import analyzer


@dataclass
class WebsitePost:
    """–î–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–∞ —Å —Å–∞–π—Ç–∞"""
    url: str
    content: str
    title: str
    timestamp: datetime
    hash: str
    selectors_found: List[str]


class HighSpeedWebsiteMonitor:
    """–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ —Å –ø–æ–∏—Å–∫–æ–º –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤"""

    def __init__(self, trading_callback=None):
        self.trading_callback = trading_callback
        self.session: Optional[aiohttp.ClientSession] = None

        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        self.content_hashes: Dict[str, str] = {}  # url -> hash –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.processed_contracts: Set[str] = set()  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã

        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        self.running = False
        self.check_interval = settings.monitoring.website_interval

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'pages_checked': 0,
            'content_changes': 0,
            'contracts_found': 0,
            'errors': 0,
            'avg_response_time': 0
        }

    async def start(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–∞–π—Ç–æ–≤"""
        if not settings.monitoring.website_urls:
            logger.warning("‚ö†Ô∏è URLs —Å–∞–π—Ç–æ–≤ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
            return False

        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTTP —Å–µ—Å—Å–∏–∏
            timeout = aiohttp.ClientTimeout(total=10)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            self.session = aiohttp.ClientSession(
                timeout=timeout,
                headers=headers
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–∞–π—Ç–æ–≤
            await self.test_websites()

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            self.running = True
            asyncio.create_task(self.monitoring_loop())

            logger.success("‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä —Å–∞–π—Ç–æ–≤ –∑–∞–ø—É—â–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∞ —Å–∞–π—Ç–æ–≤: {e}")
            return False

    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∞"""
        self.running = False
        if self.session:
            await self.session.close()
        logger.info("üõë –ú–æ–Ω–∏—Ç–æ—Ä —Å–∞–π—Ç–æ–≤ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

    async def test_websites(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–æ–≤"""
        logger.info("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–æ–≤...")

        for url in settings.monitoring.website_urls:
            if not url:
                continue

            try:
                start_time = time.time()
                async with self.session.get(url) as response:
                    response_time = (time.time() - start_time) * 1000

                    if response.status == 200:
                        logger.info(f"‚úÖ {url} –¥–æ—Å—Ç—É–ø–µ–Ω ({response_time:.0f}ms)")

                        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º hash –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
                        content = await response.text()
                        content_hash = hashlib.md5(content.encode()).hexdigest()
                        self.content_hashes[url] = content_hash

                    else:
                        logger.warning(f"‚ö†Ô∏è {url} –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è {url}: {e}")

    async def monitoring_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        logger.info(f"üîç –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–∞–π—Ç–æ–≤ (–∏–Ω—Ç–µ—Ä–≤–∞–ª: {self.check_interval}s)")

        while self.running:
            try:
                start_time = time.time()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å–∞–π—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                await self.check_all_websites()

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Ä–µ–º–µ–Ω–∏
                processing_time = time.time() - start_time
                self.stats['avg_response_time'] = (
                        self.stats['avg_response_time'] * 0.9 + processing_time * 0.1
                )

                # –ñ–¥–µ–º –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
                sleep_time = max(0, self.check_interval - processing_time)
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
                else:
                    logger.warning(f"‚ö†Ô∏è –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª: {processing_time:.1f}s")

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–∞–π—Ç–æ–≤: {e}")
                self.stats['errors'] += 1
                await asyncio.sleep(5)  # –ü–∞—É–∑–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ

    async def check_all_websites(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        tasks = []

        for url in settings.monitoring.website_urls:
            if url:
                task = asyncio.create_task(self.check_website(url))
                tasks.append(task)

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

    async def check_website(self, url: str):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è"""
        try:
            start_time = time.time()

            async with self.session.get(url) as response:
                response_time = (time.time() - start_time) * 1000

                if response.status != 200:
                    logger.debug(f"‚ö†Ô∏è {url} —Å—Ç–∞—Ç—É—Å {response.status}")
                    return

                content = await response.text()
                content_hash = hashlib.md5(content.encode()).hexdigest()

                self.stats['pages_checked'] += 1

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç
                old_hash = self.content_hashes.get(url)
                if old_hash and old_hash == content_hash:
                    # –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è
                    logger.debug(f"üìÑ {url} –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ({response_time:.0f}ms)")
                    return

                # –ö–æ–Ω—Ç–µ–Ω—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è –∏–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                self.content_hashes[url] = content_hash

                if old_hash:  # –ù–µ –ø–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                    self.stats['content_changes'] += 1
                    logger.info(f"üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞ {url} ({response_time:.0f}ms)")

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
                await self.analyze_website_content(url, content, response_time)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {url}: {e}")
            self.stats['errors'] += 1

    async def analyze_website_content(self, url: str, content: str, response_time: float):
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å–∞–π—Ç–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤"""
        try:
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(content, 'html.parser')

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = soup.get_text()

            # –ò—â–µ–º –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            contracts_from_selectors = self.extract_contracts_by_selectors(soup)

            # –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            analysis_result = await analyzer.analyze_post(
                content=page_text[:2000],  # –ü–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                platform="website",
                author=url,
                url=url
            )

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã
            all_contracts = list(set(analysis_result.addresses + contracts_from_selectors))

            if all_contracts:
                logger.critical(f"üö® –ö–û–ù–¢–†–ê–ö–¢–´ –ù–ê–ô–î–ï–ù–´ –ù–ê –°–ê–ô–¢–ï {url}")
                logger.info(f"üìç –ö–æ–Ω—Ç—Ä–∞–∫—Ç—ã: {all_contracts}")

                # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã
                new_contracts = [c for c in all_contracts if c not in self.processed_contracts]

                if new_contracts:
                    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å—Ç –¥–∞–Ω–Ω—ã–µ
                    post = WebsitePost(
                        url=url,
                        content=page_text[:500],  # –ü—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        title=soup.title.string if soup.title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞",
                        timestamp=datetime.now(),
                        hash=hashlib.md5(content.encode()).hexdigest(),
                        selectors_found=contracts_from_selectors
                    )

                    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–æ—Ä–≥–æ–≤–ª—é
                    if self.trading_callback:
                        await self.trigger_trading(new_contracts, post, analysis_result)

                    # –û—Ç–º–µ—á–∞–µ–º –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                    self.processed_contracts.update(new_contracts)
                    self.stats['contracts_found'] += len(new_contracts)

            else:
                logger.debug(f"üìÑ {url} - –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ {url}: {e}")

    def extract_contracts_by_selectors(self, soup: BeautifulSoup) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –ø–æ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º"""
        contracts = []

        for selector in settings.monitoring.website_selectors:
            try:
                elements = soup.select(selector)
                for element in elements:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–∞
                    text = element.get_text().strip()

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥—Ä–µ—Å–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    from config.settings import extract_addresses_fast, is_valid_solana_address
                    addresses = extract_addresses_fast(text)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã —ç–ª–µ–º–µ–Ω—Ç–∞
                    for attr in ['data-contract', 'data-address', 'data-token']:
                        value = element.get(attr, '').strip()
                        if value and is_valid_solana_address(value):
                            addresses.append(value)

                    contracts.extend(addresses)

            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ {selector}: {e}")

        return list(set(contracts))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

    async def trigger_trading(self, contracts: List[str], post: WebsitePost, analysis_result):
        """–ó–∞–ø—É—Å–∫ —Ç–æ—Ä–≥–æ–≤–ª–∏"""
        try:
            trading_data = {
                'platform': 'website',
                'source': post.url,
                'author': 'website',
                'url': post.url,
                'contracts': contracts,
                'confidence': analysis_result.confidence,
                'urgency': analysis_result.urgency,
                'timestamp': post.timestamp,
                'content_preview': post.content,
                'title': post.title,
                'selectors_found': post.selectors_found
            }

            # –í—ã–∑—ã–≤–∞–µ–º —Å–∏—Å—Ç–µ–º—É —Ç–æ—Ä–≥–æ–≤–ª–∏
            await self.trading_callback(trading_data)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Ç–æ—Ä–≥–æ–≤–ª–∏: {e}")

    async def health_check(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –º–æ–Ω–∏—Ç–æ—Ä–∞"""
        try:
            if not self.session:
                return {"status": "error", "message": "–°–µ—Å—Å–∏—è –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞"}

            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞
            test_url = next((url for url in settings.monitoring.website_urls if url), None)

            if test_url:
                try:
                    async with self.session.get(test_url) as response:
                        website_accessible = response.status == 200
                except:
                    website_accessible = False
            else:
                website_accessible = True  # –ù–µ—Ç —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

            return {
                "status": "healthy" if website_accessible else "degraded",
                "monitored_websites": len([url for url in settings.monitoring.website_urls if url]),
                "running": self.running,
                "website_accessible": website_accessible,
                "stats": self.stats
            }

        except Exception as e:
            return {"status": "error", "message": str(e)}

    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        return {
            **self.stats,
            "monitored_websites": len([url for url in settings.monitoring.website_urls if url]),
            "content_hashes_cached": len(self.content_hashes),
            "processed_contracts": len(self.processed_contracts),
            "running": self.running,
            "check_interval": self.check_interval
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–Ω–∏—Ç–æ—Ä–∞
website_monitor = HighSpeedWebsiteMonitor()